# Glove词向量模型

#### Q1：该模型用于解决什么样的问题？

Glove词向量模型主要用于自然语言处理领域，此前word2vector模型的提出，使得模型在学习词与词间的关系方面取得了一定进展，但它**没有利用整个语料库的全局信息**，基于此Glove诞生了。

简而言之，Glove基于语料中的全局统计信息，将词语映射至向量空间中，从而实现词语类比等任务。

#### Q2：该模型怎么实现，每一步的原理是什么？

##### Part1：共现，共现矩阵，共现概率

共现：单词 i 出现在单词 j 的上下文中（这里的上下文指以单词 j 为中心的前后各 x 个单词，原文为10个）

共现矩阵：在知晓如何计算共现次数时，我们可以通过指定上下文范围，来通过语料文本构建一个共现矩阵$X$

例如：

I like deep learning.

I like NLP.

那么此处将上下文半径设为1（即考虑中心词前，后各1个词）那么$X_{0,1}$代表第1个词在第0个词上下文中出现的次数，设第0个词为 I 第1个词为 like，则有$X_{0,1} = 2$ ,	又因为两个词之间的上下文关系是相互的（A出现在B的上下文中，则B也在A的上下文中），因此所得共现矩阵$X$是对称的。

综上，共现矩阵特点如下：

* 统计的是单词在给定环境中的共现次数，所以可以在一定程度上表达词间的关系
* 共现计数针对的是整个语料库，所以具有全局特征
* 矩阵是对称的

共现概率：由上述共现矩阵定义，可得$X_i = \sum_k X_{ik}$ 为任意词出现在词语 i 环境的次数，则有
$$
P_{ij} = P(j|i) = \frac{X_{ij}}{X_i}
$$
来表示词语 j 出现在词语 i 环境的概率，这个值称作共现概率，即在给定环境下出现某一个词的概率。

##### Part2：共现概率比

设有单词 i, j, k 则共现概率比为$ratio_{i,j,k} = \frac{p_{ik}}{p_{jk}}$

观察上式，由共现概率的计算式可知，当两个词语越相关时，概率值越接近1，所以上式的计算结果可以有一下三种情况：

* i, k相关性高，j, k相关性低，比值大于 1
* i, k相关性低，j, k相关性高，比值小于 1
* i, k相关性高，j, k相关性高 或者 i, k相关性低，j, k相关性低，比值接近1

可以发现，共现概率比的值在一定程度上体现了三个单词之间的关联，基于此作者构想，是否存在某种构造方法，使得三个词分别对应的词向量经过某种运算从而复现出对应的共现概率比呢？如果实现的话，这样的词向量就与共现矩阵有了一致性，可以体现共现矩阵的性质。

##### Part3：词向量函数

**注意：以下所有过程都是在假设结论存在的情况下反推函数构造，其过程的关键在于使得式子符合假设，不考虑是否足够严谨**

已知三个词 i, j, k，对应的词向量分别为$x_i, x_j, x_k$，目标是找到一个映射使得$f(x_i,x_j,x_k) = ratio_{i,j,k}$

由于共现概率比的值可以直接由共现矩阵得到，因此将其作为标签，通过设计模型训练的方式逼近这个值，作为一个回归问题，作者按以下方法设计。

由于等式右边是标量，为了符合，则需要对向量取转置做点积
$$
f((x_i-x_j)^T x_k) = ratio_{i,j,k}  \\
f(x_i^Tx_k - x_j^Tx_k) = ratio_{i,j,k}\\
此处x_i,x_j二者做差，可以消除共线性，体现两个向量之间的关系，与x_k做点积则代表\\衡量x_k与该关系之间的匹配程度
$$
再观察，可以发现等式右边分子是 $i, k$ 分母是 $j, k$ 若能将等式左边括号内的两项拿出来作为独立的两项，则该式可以进一步简化，那么exp函数可以满足这样的要求，基于此，上式变形为：
$$
exp(x_i^Tx_k) = P_{ik} = \frac{X_{ik}}{X_i}\\
exp(x_j^Tx_k) = P_{jk} = \frac{X_{jk}}{X_j}
$$
两边取对数则有：
$$
x_i^Tx_k=log(X_{ik})-log(x_i)\\
x_j^Tx_k=log(X_{jk})-log(x_j)
$$
以 i，k 互换为例，等式左边的值明显不变，但等式右边$log(x_i)$和$log(x_k)$不一定相等，所以上式仍需要补充，由于$x_i$是一个可以获得的常量，所以相当于在上述推到的基础上，再补充一个偏置使得式子成立，则有：
$$
x_i^Tx_k=log(X_{ik})-b_i-b_k\\
x_j^Tx_k=log(X_{jk})-b_j-b_k
$$
原问题就转化为：
$$
x_i^Tx_k + b_i+b_k=log(X_{ik})\\
其中x_i ,  x_k,b_i,b_k为待定参数
$$
损失函数设计为：
$$
J = \sum_{i = 1}^{N}\sum_{j = 1}^{N}f(X_{ij})(x_i^Tx_j+b_i+b_j-log(x_{ij}))^2
$$
其中$f(X_{ij})$为权重函数，具体如下图所示，用于控制不同大小的共现次数对结果的影响。

![](https://github.com/AI4S-Ritsuka/Postgraduate-learning-notes/blob/main/assets/NLP/Screenshot%202024-08-12%20182727.png)

这个权重函数在共现频率较高的词对上，权重会减小，从而减少这些高频词对对训练过程的影响，而在共现频率较低的词对上，权重会相对较高，从而增加这些低频词对的贡献。

